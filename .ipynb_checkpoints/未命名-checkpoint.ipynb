{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import selenium\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "import re\n",
    "import os   #<[^i]\\w+[^>]*>|</\\w*>\n",
    "PATT_PARSE_IMG=re.compile(r'<img (src=\".*?\").*?>')  #提取html中文本和图片地址\n",
    "PATT_REMOVE_HTML_PART_TAG = re.compile(r'(?!<img)(?P<ss><\\w+[^>]*>|</[a-z-]*>)')   #查找html标签（除img外）用字符串\"\"替换\n",
    "IMG_DIR_PATH = \"E:\\PythonWorkSpace\\HeadlineCrawler\\spider_data\\img\\\\\"\n",
    "Temp_Img_names = []   #[\"xxxx.png\",\"\",...]\n",
    "\n",
    "header={\n",
    "    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36',\n",
    "    'x-requested-with':'XMLHttpRequest'\n",
    "}\n",
    "\n",
    "article_dic_titel_url = {}\n",
    "def get_data(search_name,offset):\n",
    "    data = {    #构造请求的data\n",
    "        'aid':'24',\n",
    "        'app_name':'web_search',\n",
    "        'offset':offset,\n",
    "        'format':'json',\n",
    "        'keyword':search_name,\n",
    "        'autoload':'true',\n",
    "        'count':'20',\n",
    "        'en_qc':'1',\n",
    "        'cur_tab': '1',\n",
    "        'from': 'search_tab',\n",
    "        'pd':'synthesis',\n",
    "        'timestamp': int(time.time()),\n",
    "        '_signature': '21oMXgAgEBAwjHnl59qFgNtbTUAAIWq5yRBJSZ83MdD56bgu5GDIJxHd0EHk8Y1-DDSzzYJ-ZlFlc5td8NE86Wb3wfbOIt2i-9L7pr2I3.bmY8SCimmZOjMIL2g7TKFO-Lj'\n",
    "    }\n",
    "    url = 'https://www.toutiao.com/api/search/content/?' + urlencode(data)\n",
    "    res = requests.get(url=url,headers=header)\n",
    "    return res\n",
    "\n",
    "def parse_article_list(search_name,offset):\n",
    "    dic = get_data(search_name,offset).json() #转化为json字典\n",
    "    data = dic['data']\n",
    "    if data is not None:    #不为空才开始\n",
    "        for item in data:\n",
    "            if (\"video_duration_str\"not in item and \"has_video\" not in item) or item[\"has_video\"] == False:  #不需要视频文章，视频没有文字\n",
    "                if 'title' in item: #标题\n",
    "                    if 'article_url' in item:  # 文章url\n",
    "                            article_dic_titel_url[item['title']] = item['article_url']\n",
    "\n",
    "def download_img(img_url, img_to_save_url):\n",
    "    r = requests.get(img_url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        try:\n",
    "            open(img_to_save_url, 'wb').write(r.content)  # 将内容写入图片\n",
    "            return True\n",
    "        except Exception:\n",
    "            print(\"保存失败！\")\n",
    "            return False\n",
    "        finally:\n",
    "            del r\n",
    "    return False\n",
    "\n",
    "#文章爬取失败删除图片\n",
    "def del_imgs():\n",
    "    for img_name in Temp_Img_names:\n",
    "        path = IMG_DIR_PATH + img_name\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "\n",
    "#re.sub(...)函数的回调函数\n",
    "def replacement(match):\n",
    "    sentence = match.group(1)\n",
    "    if sentence.startswith(\"src\"):\n",
    "        url = sentence[5:-1]  # 获取url\n",
    "        # 下载图片到本地\n",
    "        img_name = str(time.strftime(\"%Y-%m-%d %H.%M.%S\", time.localtime()))\n",
    "        if download_img(url, IMG_DIR_PATH + img_name + \".png\"):\n",
    "            sentence = \"[图片\" + \"](.\\img\\spider_data\\\\\" + img_name + \".png)\"\n",
    "            Temp_Img_names.append(img_name + \".png\")\n",
    "        else:\n",
    "            sentence = \"[图片\" + \"](\" + url + \")\"\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def parse_article(article_url:str, browser):\n",
    "    title = \"\"\n",
    "    author=\"\"\n",
    "    release_time=\"\"\n",
    "    content=\"\"\n",
    "    if \"toutiao.com\" in article_url:  #头条站点内的文章\n",
    "        # article_url = article_url_convert(article_url)\n",
    "        browser.get(article_url)  # Load page\n",
    "        time.sleep(0.2)  # Let the page load, will be added to the API\n",
    "        try:\n",
    "            title = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/h1\").text\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            title = \"\"\n",
    "\n",
    "        try:\n",
    "            release_time = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/div[1]/span[3]\").text\n",
    "            author = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/div[1]/span[2]\").text\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            try:\n",
    "                release_time = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/div[1]/span[2]\").text\n",
    "                author = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/div[1]/span[1]\").text\n",
    "            except selenium.common.exceptions.NoSuchElementException:\n",
    "                release_time = \"\"\n",
    "                author = \"\"\n",
    "\n",
    "        try:\n",
    "            content_tag = browser.find_element_by_xpath(\"/html/body/div/div[2]/div[2]/div[1]/article\")\n",
    "            content_html = content_tag.get_attribute(\"innerHTML\")\n",
    "            content_html = re.sub(PATT_REMOVE_HTML_PART_TAG, \"\", content_html)  #消除除img以外的标签\n",
    "            content = re.sub(PATT_PARSE_IMG, replacement, content_html)\n",
    "\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            content = \"\"\n",
    "\n",
    "        # https: // www.toutiao.com / a6794358241716339211 /\n",
    "    else:\n",
    "        None\n",
    "    return article_url, title, author, release_time, content\n",
    "\n",
    "def my_test():\n",
    "    \n",
    "    article_url, title, author, release_time, content = parse_article(\"https://www.toutiao.com/a6784351002280591876/\", browser)\n",
    "    print(content)\n",
    "    browser.close()\n",
    "    # print(download_img(\"https://p6-tt.byteimg.com/origin/pgc-image/d192ba55afe4411c805b97c5d4c127f0?from=pc\",'C:/Users/Administrator/Desktop/'+str(time.strftime(\"%Y-%m-%d %H.%M.%S\", time.localtime())) + \".png\"))\n",
    "\n",
    "\n",
    "def main(keyword):\n",
    "    Temp_Img_names = []\n",
    "    #获取相关文章url链接\n",
    "    print(\"\\n↓↓↓↓↓↓↓↓↓爬取文章url↓↓↓↓↓↓↓↓↓\\n\")\n",
    "    if len(article_dic_titel_url) == 0:\n",
    "        offset = 0\n",
    "        for i in tqdm(range(11)):   #首页列表中只有98条（只包括文章），包含视频有180条\n",
    "            parse_article_list(keyword, offset)\n",
    "            offset = offset + 20\n",
    "    try:\n",
    "        print(article_dic_titel_url)\n",
    "        browser = webdriver.Chrome()  # Get local session of Chrome\n",
    "        all_data = {}\n",
    "        all_data[\"url\"] = []\n",
    "        all_data[\"title\"] = []\n",
    "        all_data[\"author\"] = []\n",
    "        all_data[\"release_time\"] = []\n",
    "        all_data[\"content\"] = []\n",
    "        print(\"\\n↓↓↓↓↓↓↓↓↓爬取文章详情↓↓↓↓↓↓↓↓↓\\n\")\n",
    "        count = 0\n",
    "        for t, url in tqdm(article_dic_titel_url.items()):  #解析所有文章\n",
    "            article_url, title, author, release_time, content = parse_article(url, browser)\n",
    "            all_data[\"url\"].append(article_url)\n",
    "            all_data[\"title\"].append(title)\n",
    "            all_data[\"author\"].append(author)\n",
    "            all_data[\"release_time\"].append(release_time)\n",
    "            all_data[\"content\"].append(content)\n",
    "            if content != \"\":\n",
    "                count += 1\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(\"spider_data/\"+keyword+\"_\"+str(count)+\"条_articles_info_\"+str(time.strftime(\"%Y-%m-%d %H.%M.%S\", time.localtime()))+\".csv\")\n",
    "    except Exception:\n",
    "        print(\"保存失败\")\n",
    "        del_imgs()\n",
    "    finally:\n",
    "        browser.close()\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"无人船\")\n",
    "    # my_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import selenium\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_news():\n",
    "    data = {    #构造请求的data\n",
    "        'aid':'24',\n",
    "        'app_name':'web_search',\n",
    "        'offset':offset,\n",
    "        'format':'json',\n",
    "        'keyword':search_name,\n",
    "        'autoload':'true',\n",
    "        'count':'20',\n",
    "        'en_qc':'1',\n",
    "        'cur_tab': '1',\n",
    "        'from': 'search_tab',\n",
    "        'pd':'synthesis',\n",
    "        'timestamp': int(time.time()),\n",
    "        '_signature': '21oMXgAgEBAwjHnl59qFgNtbTUAAIWq5yRBJSZ83MdD56bgu5GDIJxHd0EHk8Y1-DDSzzYJ-ZlFlc5td8NE86Wb3wfbOIt2i-9L7pr2I3.bmY8SCimmZOjMIL2g7TKFO-Lj'\n",
    "    }\n",
    "    url = 'https://www.toutiao.com/api/search/content/?' + urlencode(data)\n",
    "    res = requests.get(url=url,headers=header)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadLine:\n",
    "    \"\"\"\n",
    "    topic\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.browser = selenium.webdriver.Chrome()\n",
    "        self.base_dir = '/User/vito/spider/'\n",
    "        \n",
    "    def get_page(self, topic:str, offset:int):\n",
    "        #通过decode工具得到data\n",
    "        params = {\n",
    "        'aid':'24',\n",
    "        'app_name':'web_search',\n",
    "        'offset':offset,\n",
    "        'format':'json',\n",
    "        'keyword':search_name,\n",
    "        'autoload':'true',\n",
    "        'count':'20',\n",
    "        'en_qc':'1',\n",
    "        'cur_tab': '1',\n",
    "        'from': 'search_tab',\n",
    "        'pd':'synthesis',\n",
    "        'timestamp': int(time.time()),\n",
    "        '_signature': 'IbFHAAAgEBA3yUF75cndAiGwBhAAH7IJXfv09r0xnfIHXUO.wGRyqhIENMKOMyAu3iCJztEo-qyssj6DE9odGsrrvM9bgxjb8OTCJCZ2keUE1waSz2iIaY7SpaNVulSvUp5'\n",
    "        }\n",
    "    url = 'https://www.toutiao.com/api/search/content/?'\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params)\n",
    "        if response.status_code==200:\n",
    "            return response.json()\n",
    "    except requests.ConnectionError:\n",
    "        return None\n",
    "    \n",
    "    def __call__(self, topic:str):\n",
    "        data_dir = os.path.join(self.base_dir,topic)\n",
    "        if not os.path.isdiri(data_dir):\n",
    "            os.mkdir(data_dir)\n",
    "        offset = 0\n",
    "        url = get_url(self, topic, offset)\n",
    "        response = requests.get(url, header)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00, 36.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取热点文章链接\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 36.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import selenium\n",
    "\n",
    "browser = webdriver.Chrome()            # 创建浏览器对象\n",
    "browser.get('http://news.sina.com.cn/hotnews/')\n",
    "time.sleep(1)\n",
    "browser.find_element_by_xpath('//*[@id=\"Tab12\"]').click()\n",
    "article_list = browser.find_elements_by_xpath('//*[@id=\"Con12\"]/table/tbody/tr')\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "content_href = []\n",
    "comment_href = []\n",
    "num_comments = []\n",
    "comments = []\n",
    "\n",
    "print(\"获取热点文章链接\")\n",
    "\n",
    "for i in tqdm(range(1,11)):\n",
    "    item = article_list[i].find_elements_by_tag_name('a')\n",
    "    content_href.append(item[0].get_attribute('href'))\n",
    "    titles.append(item[0].get_attribute('textContent'))\n",
    "    comment_href.append(item[1].get_attribute('href'))\n",
    "    num_comments.append(item[1].get_attribute('textContent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url):\n",
    "    # selenium 模拟\n",
    "    browser.get(url)\n",
    "    if 'ent' in url:\n",
    "        try:\n",
    "            content_list = browser.find_elements_by_xpath('//*[@id=\"artibody\"]/p')\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            content_list = []\n",
    "        \n",
    "    if 'finance' in url:\n",
    "        try:\n",
    "            content_list = browser.find_elements_by_xpath('//*[@id=\"artibody\"]/p')\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            content_list = []\n",
    "    if 'news' in url:\n",
    "        try:\n",
    "            content_list = browser.find_elements_by_xpath('//*[@id=\"article\"]/p')\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            content_list = []\n",
    "    news_content = ''.join([c.text for c in content_list if c.text is not None])\n",
    "    return news_content\n",
    "    \n",
    "def get_comment(url):\n",
    "    # selenium 模拟\n",
    "    browser.get(url)\n",
    "    bottom = None\n",
    "    while bottom is None:\n",
    "        try:\n",
    "            bottom = browser.find_element_by_xpath(\"//*[text()='更多精彩评论>>']\")\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            bottom = None\n",
    "        time.sleep(1)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    try:\n",
    "        comment_list = browser.find_elements_by_class_name('txt')\n",
    "    except selenium.common.exceptions.NoSuchElementException:\n",
    "        comment_list = []\n",
    "    comment_list = [c.text for c in comment_list if c.text is not None]\n",
    "    return comment_list\n",
    "\n",
    "for url in tqdm(content_href):\n",
    "    contents.append(get_content(url))\n",
    "    time.sleep(random.randint(1,3))\n",
    "\n",
    "for url in tqdm(comment_href):\n",
    "    comments.append(get_comment(url)\n",
    "    break\n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:16<?, ?it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(topic:str, offset:int):\n",
    "        params = {\n",
    "        'aid':'24',\n",
    "        'app_name':'web_search',\n",
    "        'offset':offset,\n",
    "        'format':'json',\n",
    "        'keyword':topic,\n",
    "        'autoload':'true',\n",
    "        'count':'20',\n",
    "        'en_qc':'1',\n",
    "        'cur_tab': '1',\n",
    "        'from': 'search_tab',\n",
    "        'pd':'synthesis',\n",
    "        'timestamp': int(time.time()),\n",
    "#         '_signature': 'IbFHAAAgEBA3yUF75cndAiGwBhAAH7IJXfv09r0xnfIHXUO.wGRyqhIENMKOMyAu3iCJztEo-qyssj6DE9odGsrrvM9bgxjb8OTCJCZ2keUE1waSz2iIaY7SpaNVulSvUp5'\n",
    "        }\n",
    "        url = 'https://www.toutiao.com/api/search/content/?'\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params)\n",
    "            if response.status_code==200:\n",
    "                return response.json()\n",
    "        except requests.ConnectionError:\n",
    "            return None\n",
    "\n",
    "get_page('奶茶', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# 进入爬取页面\n",
    "def search():\n",
    "    try:\n",
    "        url = 'https://news.sina.com.cn/roll/#pageid=153&lid=2509&k=&num=50&page=1'\n",
    "        browser.get(url)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, 'pL_Main')))\n",
    "        getDetail()\n",
    "        total = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#d_list > div > span:nth-child(14) > a')))\n",
    "        return total.text\n",
    "    except TimeoutError:\n",
    "        return search()\n",
    "# 得到具体信息\n",
    "def getDetail():\n",
    "    html = pq(browser.page_source,parser=\"html\")\n",
    "    content = html.find('#d_list')\n",
    "    uls = content.find('ul').items()\n",
    "    for ul in uls:\n",
    "        lis = ul('li').items()\n",
    "        for li in lis:\n",
    "            news = {\n",
    "                'title': li.find('.c_tit a').text(),\n",
    "                'href': li.find('.c_tit a').attr('href'),\n",
    "                'time': li.find('.c_time').text()\n",
    "            }\n",
    "            print(news)\n",
    "# 爬取下一页\n",
    "def next_detail(page_number):\n",
    "    try:\n",
    "        nextBotton = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '#d_list > div > span:last-child > a')))\n",
    "        nextBotton.click()\n",
    "        wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#d_list > div > span.pagebox_num_nonce'), str(page_number)))\n",
    "        getDetail()\n",
    "    except TimeoutException:\n",
    "        next_detail(page_number)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        total = search()\n",
    "        total = int(total)\n",
    "        for i in range(2, total + 1):\n",
    "            next_detail(i)\n",
    "    except Exception:\n",
    "        print(Exception)\n",
    "    finally:\n",
    "        browser.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "#存入本地txt\n",
    "def write_article(data,flag):\n",
    "    if flag == 1:\n",
    "        file_name = 'newscontent.txt'\n",
    "        f = open(file_name, 'a', encoding='utf-8')\n",
    "        f.write(data)\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.close()\n",
    "    elif flag == 2:\n",
    "        file_name = 'newscomment.csv'\n",
    "        name = ['comment','agree']\n",
    "        comments = pd.DataFrame(list(zip(*data)),columns=name)\n",
    "        comments.to_csv(file_name)\n",
    "    else:\n",
    "        file_name = 'newscomment.txt'\n",
    "        f = open(file_name, 'a', encoding='utf-8')\n",
    "        f.write(data)\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        html = urllib.request.urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        obj = BeautifulSoup(html.read(),\"html.parser\")\n",
    "    except HTTPError as f:\n",
    "        return None\n",
    "    nameList = obj.findAll(\"div\", {\"class\": \"article\"})\n",
    "    for name in nameList:\n",
    "        print(name.get_text())\n",
    "        str = name.get_text()\n",
    "        write_article(str,1)\n",
    "get_content(\"https://finance.sina.com.cn/chanjing/gsnews/2019-12-11/doc-iihnzahi6659560.shtml\")\n",
    "\n",
    "\n",
    "\n",
    "#定义空数组 用来存放评论和评论获赞的数量\n",
    "listAll=[]\n",
    "listComments = []\n",
    "listAgree = []\n",
    "\n",
    "#获取当前页的评论 最多三条热门\n",
    "comments = requests.get(\"https://comment.sina.com.cn/page/info?version=1&format=json&channel=cj&newsid=comos-ihnzahi6659560&group=undefined&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=3&t_size=3&h_size=3&thread=1&uid=unlogin_user\")\n",
    "comments.encoding=('utf-8')\n",
    "comments.text\n",
    "jd = json.loads(comments.text)\n",
    "#print(jd)\n",
    "for x in range(3):\n",
    "    print(jd['result']['hot_list'][x]['content'])\n",
    "    str1 = jd['result']['hot_list'][x]['content']\n",
    "    str2 = jd['result']['hot_list'][x]['agree']\n",
    "    write_article(str1,3)\n",
    "    listComments.append(str1)\n",
    "    listAgree.append(str2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#打开更多评论查看全部评论\n",
    "comments2 = requests.get(\"http://comment.sina.com.cn/page/info?version=1&format=json&channel=cj&newsid=comos-ihnzahi6659560&group=0&compress=0&ie=utf-8&oe=utf-8&page=1&page_size=10&t_size=3&h_size=3&thread=1&uid=unlogin_user\")\n",
    "comments2.encoding=('utf-8')\n",
    "comments.text\n",
    "jd1 = json.loads(comments2.text)\n",
    "count = jd1['result']['count']['thread_show']\n",
    "#print(jd1)\n",
    "\n",
    "#-5防止下标越界\n",
    "for x in range(count-5):\n",
    "    print(jd1['result']['cmntlist'][x]['content'])\n",
    "    str1 = jd1['result']['cmntlist'][x]['content']\n",
    "    str2 = jd1['result']['cmntlist'][x]['agree']\n",
    "    write_article(str1,3)\n",
    "    listComments.append(str1)\n",
    "    listAgree.append(str2)\n",
    "\n",
    "#把评论和赞放到一个数组里面\n",
    "listAll.append(listComments)\n",
    "listAll.append(listAgree)\n",
    "\n",
    "\n",
    "#将评论写入cvs文件\n",
    "#print(listAll)\n",
    "write_article(listAll,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snownlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"newscomment.csv\")\n",
    "data.head()\n",
    "#情感划分 赞大于300的给1 赞小于三百的给0\n",
    "def make_label(agree):\n",
    " if agree > 300:\n",
    "    return 1\n",
    " else:\n",
    "    return 0\n",
    "\n",
    "data['sentiment'] = data.agree.apply(make_label)\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n",
    "#结巴分词\n",
    "def chinese_word_cut(mytext):\n",
    " return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "data['cut_comment'] = data.comment.apply(chinese_word_cut)\n",
    "\n",
    "#划分数据集\n",
    "X = data['cut_comment']\n",
    "y = data.sentiment\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "\n",
    "#数据处理\n",
    "def get_custom_stopwords(stop_words_file):\n",
    " with open(stop_words_file,'r',encoding='utf-8') as f:\n",
    "    stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    " return custom_stopwords_list\n",
    "\n",
    "stop_words_file = '哈工大停用词表.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "\n",
    "vect = CountVectorizer(max_df = 0.8,\n",
    "                       min_df = 3,\n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "test = pd.DataFrame(vect.fit_transform(X_train).toarray(), columns=vect.get_feature_names())\n",
    "test.head()\n",
    "\n",
    "\n",
    "#训练模式  朴素贝叶斯算法\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "#print(train_score)\n",
    "\n",
    "#测试数据\n",
    "X_test_vect = vect.transform(X_test)\n",
    "#print(nb.score(X_test_vect, y_test))\n",
    "\n",
    "#使用snownlp插件包\n",
    "data = open(\"newscomment.txt\",encoding='utf-8')\n",
    "s = data.read()\n",
    "snownlp = SnowNLP(s)\n",
    "print(snownlp.sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlconda34f4c0e8019f49848bc5a3e96f43aeac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
